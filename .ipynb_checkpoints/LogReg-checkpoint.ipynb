{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 2000) # since we have a lot of features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import data '''\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "train_file = os.path.join(DATA_DIR, 'train.csv')\n",
    "test_file = os.path.join(DATA_DIR, 'test.csv')\n",
    "submission_file = os.path.join(DATA_DIR, 'sample_submission.csv')\n",
    "\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "X = train.drop(columns=['y', 'sample_id'])\n",
    "y = train.y\n",
    "X_test = test.drop(columns=['sample_id'])\n",
    "\n",
    "X_all = X.append(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(766, 1777)\n",
      "(766, 932)\n",
      "(766, 1908)\n",
      "(766, 1908)\n",
      "(329, 2518)\n",
      "(329, 1908)\n",
      "(727, 1908)\n"
     ]
    }
   ],
   "source": [
    "''' Gather all preprocessing functions '''\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import bisect\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "''' 1 - divide to training and holdout datasets '''\n",
    "\n",
    "X_train, X_hold, y_train, y_hold = train_test_split(X, y, random_state=3, test_size=0.3)\n",
    "\n",
    "''' 2 - define the same validation scheme for all models '''\n",
    "\n",
    "skf = RepeatedStratifiedKFold(n_repeats=5, random_state=17)\n",
    "\n",
    "\n",
    "''' Function 1 - delete all columns with NaN or Inf '''\n",
    "columns_to_keep = X_all.replace([np.inf, -np.inf], np.nan).dropna(axis=1).columns\n",
    "\n",
    "def del_nan(x):\n",
    "    return x[columns_to_keep]\n",
    "delete_nan = FunctionTransformer(del_nan, validate=False)\n",
    "\n",
    "\n",
    "''' Function 2 - delete NaN-only columns '''\n",
    "\n",
    "na_cols = X_all[X_all.isna().any()[lambda x: x].index].isna().agg(['sum', 'count']).T\n",
    "\n",
    "all_na_cols = na_cols[na_cols['sum'] == na_cols['count']]\n",
    "\n",
    "def drop_all_na(X):\n",
    "    ''' Here we drop all columns which consist of NaN only '''\n",
    "    idx = all_na_cols.index.values\n",
    "    return X.drop(columns=idx)\n",
    "\n",
    "drop_all_na_f = FunctionTransformer(drop_all_na, validate=False)\n",
    "\n",
    "\n",
    "''' Function 3 - replace NaN '''\n",
    "\n",
    "has_na_cols = na_cols.T.drop(columns=all_na_cols.index.values).T\n",
    "has_na_cols['perc'] = has_na_cols['sum']/has_na_cols['count']\n",
    "\n",
    "'''def replace_na(X, add_boolean=True, threshold=0.3):\n",
    "    Here we apply two filling strategies\n",
    "            for two types of columns with na:\n",
    "            % na > threshold - fill with mean\n",
    "            % na < threshold - fill with 0.\n",
    "        Plus we add additional columns showing\n",
    "            if the value was na before transformation.\n",
    "    X_copy = X.copy()\n",
    "    cols_less = has_na_cols[has_na_cols['perc'] < threshold].index.values\n",
    "    cols_more = has_na_cols[has_na_cols['perc'] >= threshold].index.values\n",
    "    X_less = X_copy[cols_less]\n",
    "    X_less = X_less.fillna(X_less.mean())\n",
    "    X_copy[cols_less] = X_less\n",
    "    X_copy[cols_more] = X_more\n",
    "    if add_boolean:\n",
    "        was_na = X[has_na_cols.index.values].isna().astype('int64')\n",
    "        was_na.columns = was_na.columns.map(lambda x: x + '_na')\n",
    "        X_copy = pd.concat([X_copy, was_na], axis=1)\n",
    "    return X_copy'''\n",
    "    \n",
    "# I should have checked columns for categorical features before\n",
    "\n",
    "def replace_na(X, add_boolean=True):\n",
    "    ''' Here we apply mean filling strategy.\n",
    "        And add additional columns showing if the values\n",
    "        was na before transformation.\n",
    "    '''\n",
    "    X_copy = X.copy()\n",
    "    cols = has_na_cols.index.values\n",
    "    X_c = X_copy[cols]\n",
    "    X_c = X_c.fillna(X_all.replace([np.inf, -np.inf], np.nan).mean())\n",
    "    X_copy[cols] = X_c\n",
    "    if add_boolean:\n",
    "        was_na = X[has_na_cols.index.values].isna().astype('int64')\n",
    "        was_na.columns = was_na.columns.map(lambda x: x + '_na')\n",
    "        X_copy = pd.concat([X_copy, was_na], axis=1)\n",
    "    return X_copy\n",
    "\n",
    "replace_na_f = FunctionTransformer(replace_na, validate=False)\n",
    "\n",
    "\n",
    "''' Function 4 - replace inf '''\n",
    "\n",
    "pos_inf_cols = X_all.fillna(0).replace([np.inf], \n",
    "                                       np.nan).isna().any()[lambda x: x].index\n",
    "pos_inf_cols = X_all[pos_inf_cols].replace([np.inf], \n",
    "                                           np.nan).isna().agg(['sum', 'count']).T\n",
    "neg_inf_cols = X_all.fillna(0).replace([-np.inf],\n",
    "                                       np.nan).isna().any()[lambda x: x].index\n",
    "\n",
    "def fill_inf(X, add_boolean=True, multiplier=2):\n",
    "    '''\n",
    "    Here we apply filling strategy for infinite values:\n",
    "        we replace it with absolute maximum value * multiplier.\n",
    "    Additionally, we new columns showing if the value\n",
    "        was infinite before filling.\n",
    "    '''\n",
    "    X_copy = X.copy()\n",
    "    X_pos = X_copy[pos_inf_cols.index.values]\n",
    "    to_fill = X_pos.replace(np.inf, np.nan).abs().max() * multiplier + 1.0\n",
    "    X_pos = X_pos.replace(np.inf, np.nan).fillna(to_fill)\n",
    "    X_copy[pos_inf_cols.index.values] = X_pos\n",
    "    if add_boolean:\n",
    "        was_inf = X[pos_inf_cols.index.values].replace(np.inf, np.nan).isna().astype('int64')\n",
    "        was_inf.columns = was_inf.columns.map(lambda x: x + '_inf')\n",
    "        X_copy = pd.concat([X_copy, was_inf], axis=1)\n",
    "    return X_copy\n",
    "\n",
    "fill_inf_f = FunctionTransformer(fill_inf, validate=False)\n",
    "\n",
    "\n",
    "''' First block of data cleaning is gathered in one function '''\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "\n",
    "''' Function and class 5 - delete zero-variance features '''\n",
    "\n",
    "w = fill_inf(replace_na(drop_all_na(X_all))).var()\n",
    "zero_var_cols = w[np.isclose(w, 0)]\n",
    "\n",
    "def drop_zero_var(X):\n",
    "    idx = zero_var_cols.index.values\n",
    "    return X.drop(columns=idx)\n",
    "\n",
    "drop_zero_var_f = FunctionTransformer(drop_zero_var)\n",
    "\n",
    "class Drop_zero_var:\n",
    "    def __init__(self):\n",
    "        self.zero_var_cols = []\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        w = X.var()\n",
    "        self.zero_var_cols = w[np.isclose(w, 0.0)].index.values\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        return X_copy.drop(columns=self.zero_var_cols)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(Drop_zero_var().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "\n",
    "''' Function and class 6 - drop highly correlated features '''\n",
    "\n",
    "'''\n",
    "# Here is the function\n",
    "print(drop_zero_var(pre_proc(X_all)).shape)\n",
    "corr_matrix = pre_proc(X_all).corr().abs()\n",
    "print(corr_matrix.shape)\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(len(to_drop))\n",
    "\n",
    "'''\n",
    "\n",
    "class Drop_corr:\n",
    "    def __init__(self, threshold=0.95):\n",
    "        self.to_drop = []\n",
    "        self.is_fitted = False\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        corr_matrix = X.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        self.to_drop = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        return X_copy.drop(columns=self.to_drop)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(Drop_corr().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "    \n",
    "''' Class 7 - encode categorical labels '''\n",
    "\n",
    "class LE_df:\n",
    "    def __init__(self, threshold=5):\n",
    "        self.le_dict = {}\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        categorical = X_copy[counts[counts <= self.threshold].index]\n",
    "        for col in categorical.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(categorical[col])\n",
    "            self.le_dict[col] = le\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.le_dict.keys():\n",
    "            X_copy[col] = X_copy[col].map(lambda s: -100500 if s not in \\\n",
    "                                          self.le_dict[col].classes_ else s)\n",
    "            if np.any(X_copy[col]==-100500):\n",
    "                le_classes = self.le_dict[col].classes_.tolist()\n",
    "                le_classes.append(-100500)\n",
    "                self.le_dict[col].classes_ = np.asarray(le_classes)\n",
    "            X_copy[col] = self.le_dict[col].transform(X_copy[col])\n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "print(LE_df().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "    \n",
    "''' Class 8 - target mean encode categorical labels '''\n",
    "\n",
    "class LOOTME_df:\n",
    "    ''' Class performs leave-one-out target mean encoding '''\n",
    "    def __init__(self, threshold=5):\n",
    "        self.threshold = threshold\n",
    "        self.tme_dict = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_copy = X.copy()\n",
    "        y_copy = y.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        categorical = X_copy[counts[counts <= self.threshold].index]\n",
    "        for col in categorical.columns:\n",
    "            df = pd.concat((X_copy[col], y_copy), axis=1)\n",
    "            df.columns = ['col', 'y']\n",
    "            tme = {}\n",
    "            for value in df['col'].unique():\n",
    "                y_val = df[df['col'] == value]['y'].values\n",
    "                col_val = df[df['col'] == value]['col'].values\n",
    "                loo_mean = np.mean((np.full_like(col_val, y_val.sum()) - y_val) / y_val.shape[0])\n",
    "                tme[value] = loo_mean\n",
    "            self.tme_dict[col] = tme\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.tme_dict.keys():\n",
    "            X_copy[col] = X_copy[col].map(lambda s: -100500 if s not in self.tme_dict[col].keys() else s)\n",
    "            self.tme_dict[col][-100500] = 0.0\n",
    "            X_copy[col] = X_copy[col].map(self.tme_dict[col])\n",
    "        return X_copy\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "    \n",
    "print(LOOTME_df().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "\n",
    "\n",
    "''' Class 9 - One Hot Encoding of categorical features '''\n",
    "\n",
    "class OneHot_df:\n",
    "    def __init__(self, threshold=5, drop_collinear=False):\n",
    "        self.threshold = threshold\n",
    "        self.drop_collinear = drop_collinear\n",
    "        self.categorical = []\n",
    "        self.ohe = None\n",
    "        self.column_names = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        X_cat = X_copy[counts[(counts <= self.threshold) & (counts > 1)].index]\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        ohe.fit(X_cat)\n",
    "        self.categorical = X_cat.columns\n",
    "        self.ohe = ohe\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_cat = X_copy.copy()[self.categorical]\n",
    "        X_cat_tr = self.ohe.transform(X_cat).toarray()\n",
    "        if not self.column_names:\n",
    "            column_names = []\n",
    "            for i, col in enumerate(self.categorical):\n",
    "                column_names.extend([col + '_' + str(s) for s in self.ohe.categories_[i]])\n",
    "            self.column_names = column_names\n",
    "        X_cat_tr = pd.DataFrame(X_cat_tr, columns=self.column_names, index=X_copy.index)\n",
    "        X_other = X_copy.drop(columns=self.categorical)\n",
    "        X_copy = pd.concat([X_other, X_cat_tr], axis=1)\n",
    "        if self.drop_collinear:\n",
    "            to_drop = pd.Series(self.column_names, \n",
    "        index=self.column_names).apply(lambda x: x.endswith('_0'))[lambda x: x].index\n",
    "            X_copy.drop(columns=to_drop)\n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(OneHot_df().fit(pre_proc(X_train)).transform(pre_proc(X_hold)).shape)\n",
    "\n",
    "''' Class 10 - find best normalizing transformation '''\n",
    "\n",
    "def get_shapiro_p(X):\n",
    "    _, p = shapiro(X)\n",
    "    return p\n",
    "\n",
    "class Find_Trans:\n",
    "    def __init__(self, threshold=10):\n",
    "        self.threshold = threshold\n",
    "        self.pvals = []\n",
    "        self.numerical = []\n",
    "        self.is_fitted = False\n",
    "        self.mms = MinMaxScaler()\n",
    "        self.trans_dict = {'no': lambda x: x,\n",
    "                           'x^2': lambda x: np.power(x, 2), \n",
    "                           'x^3': lambda x: np.power(x, 3), \n",
    "                           'log(x)': lambda x: np.log(x), \n",
    "                           #'exp(x)': lambda x: np.exp(x/10), \n",
    "                           'sqrt(x)': lambda x: np.sqrt(x)}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        self.numerical = counts[counts > self.threshold].index.values\n",
    "        X_numer = X_copy[self.numerical]\n",
    "        idx = X_numer.index\n",
    "        X_numer = pd.DataFrame(self.mms.fit_transform(X_numer) + 0.01,\n",
    "                               columns=self.numerical,\n",
    "                               index=idx)\n",
    "        pvals = pd.DataFrame(X_numer.apply(get_shapiro_p), columns=['no'])\n",
    "        for name in self.trans_dict.keys():\n",
    "            if name not in pvals.columns:\n",
    "                pvals[name] = X_numer.apply(self.trans_dict[name]).apply(get_shapiro_p)\n",
    "        self.pvals = pvals\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_numer = X_copy[self.numerical]\n",
    "        idx = X_numer.index\n",
    "        X_numer = pd.DataFrame(self.mms.transform(X_numer) + 0.01,\n",
    "                               columns=self.numerical,\n",
    "                               index=idx)\n",
    "        best_transform = self.pvals.idxmax(axis=1)\n",
    "        best_transform = best_transform.map(self.trans_dict).to_dict()\n",
    "        X_numer = X_numer.apply(best_transform)\n",
    "        X_numer = X_numer.fillna(X_numer.min()-1)\n",
    "        for col in self.numerical:\n",
    "            X_copy[col] = X_numer[col]\n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(Find_Trans().fit(pre_proc(X_train), y_train).transform(pre_proc(X_hold)).shape)\n",
    "\n",
    "\n",
    "''' Class 11 - find and drop outliers '''\n",
    "\n",
    "class DetectOut_df:\n",
    "    def __init__(self, method='IsolationForest', out_fraction=0.05, \n",
    "                 drop=True, random_state=17,\n",
    "                 kernel='rbf', shrink=True):\n",
    "        self.method = method\n",
    "        self.out_fraction = out_fraction\n",
    "        self.drop = drop\n",
    "        self.is_fitted = False\n",
    "        self.random_state = random_state\n",
    "        self.kernel = kernel\n",
    "        self.shrink = shrink\n",
    "        self.model = None\n",
    "        self.train_set = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'IsolationForest':\n",
    "            model = IsolationForest(random_state=self.random_state,\n",
    "                        n_estimators=100,\n",
    "                        max_features=0.9,\n",
    "                        contamination=self.out_fraction, #\n",
    "                        n_jobs=-1,\n",
    "                        behaviour='new')\n",
    "        if self.method == 'OneClassSVM':\n",
    "            model = OneClassSVM(kernel=self.kernel,\n",
    "                     nu=self.out_fraction,\n",
    "                     shrinking=self.shrink,\n",
    "                     gamma='scale'\n",
    "                    )\n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        self.is_fitted = True\n",
    "        self.train_set = X.copy()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        ''' We shouldn't predict any outliers for test set '''\n",
    "        X_copy = X.copy()\n",
    "        X_copy['outlier'] = 1\n",
    "        X_copy['y'] = y.copy()\n",
    "        if self.train_set.equals(X):\n",
    "            X_copy['outlier'] = self.model.predict(X)\n",
    "        if self.drop:\n",
    "            X_copy = X_copy[X_copy['outlier'] == 1]\n",
    "            X_copy = X_copy.drop(columns=['outlier'])\n",
    "        y_copy = X_copy['y']\n",
    "        X_copy = X_copy.drop(columns=['y'])\n",
    "        return X_copy, y_copy\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "    \n",
    "print(DetectOut_df().fit_transform(pre_proc(X_train), y_train)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель 1 - те же стадии препроцессинга, что и на дереве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.9317\n",
      "Cross-validation ROC AUC: mean 0.8290, std 0.0211\n",
      "Holdout ROC AUC: 0.8519\n"
     ]
    }
   ],
   "source": [
    "''' Let's try logreg on our features and compare preprocessing steps '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=1e-4,\n",
    "                        alpha=0.001)\n",
    "\n",
    "pipe_log = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      #('le', LE_df(threshold=CAT_THRESHOLD)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      #('sc', StandardScaler()),\n",
    "                      ('SGDlog', sgd_log)])\n",
    "pipe_log.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(pipe_log, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train, pipe_log.predict_proba(X_train)[:,1]))\n",
    "print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, pipe_log.predict_proba(X_hold)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param = ['all', 'no_sc', 'no_transform_no_sc', 'no_lootme_no_sc', 'le_insteadof_lootme_no_sc']\n",
    "\n",
    "CV_ROC_AUC = [0.7611, 0.8291, 0.6834, 0.8198, 0.8212]\n",
    "\n",
    "HOLD_ROC_AUC = [0.7891, 0.8363, 0.6123, 0.8494, 0.8442]\n",
    "\n",
    "Лучшая схема препроцессинга - та же, что и для дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров - лучшие параметры внизу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "''' Let's try to find best parameters '''\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "n_iter = 30\n",
    "random_state = 17\n",
    "\n",
    "from hyperopt import fmin, Trials, hp, tpe\n",
    "\n",
    "def log_roc_cv(params, random_state=random_state, cv=skf, X=X_train, y=y_train):\n",
    "    # the function gest a set of variable parameters in \"param\"\n",
    "    params = {'alpha': params['alpha'], \n",
    "              'l1_ratio': params['l1_ratio'], \n",
    "              'tol': params['tol']}\n",
    "    \n",
    "    CORR_THRESHOLD = 0.94\n",
    "    CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "\n",
    "    def pre_proc(X):\n",
    "        return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "    sgd_log = SGDClassifier(loss='log', random_state=random_state, **params)\n",
    "\n",
    "    pipe_log = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      ('SGDlog', sgd_log)])\n",
    "    \n",
    "    # and then conduct the cross validation with the same folds as before\n",
    "    score = -cross_val_score(pipe_log, X, y, cv=cv, scoring=\"roc_auc\", n_jobs=-1).mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# possible values of parameters\n",
    "space={'alpha': hp.loguniform('alpha', -4, 2),\n",
    "       'l1_ratio' : hp.quniform('l1_ratio', 0.0, 0.5, 0.02),\n",
    "       'tol': hp.loguniform('tol', -5, -2)\n",
    "      }\n",
    "\n",
    "# trials will contain logging information\n",
    "trials = Trials()\n",
    "\n",
    "\n",
    "best=fmin(fn=log_roc_cv, # function to optimize\n",
    "          space=space, \n",
    "          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n",
    "          max_evals=n_iter, # maximum number of iterations\n",
    "          trials=trials, # logging\n",
    "          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n",
    "         )\n",
    "\n",
    "# computing the score on the test set\n",
    "model = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      ('tree', SGDClassifier(loss='log', \n",
    "                                             n_jobs=-1, \n",
    "                                             random_state=random_state, \n",
    "                                             alpha=best['alpha'],\n",
    "                                             l1_ratio=best['l1_ratio'],\n",
    "                                             tol=best['tol']))])\n",
    "model.fit(X_train, y_train)\n",
    "tpe_test_score = roc_auc_score(y_hold, model.predict_proba(X_hold)[:, 1])\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]))\n",
    "print(\"Best ROC AUC {:.4f} params {}\".format( -log_roc_cv(best), best))\n",
    "print('Holdout ROC AUC: %.4f' % tpe_test_score)\n",
    "\"\"\"\n",
    "\n",
    "# best parameters: alpha=0.0193, l1_ratio=0.12, tol=3e-2. Best score: cv 0.8234, hold 0.8474."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.8790\n",
      "Cross-validation ROC AUC: mean 0.8230, std 0.0217\n",
      "Holdout ROC AUC: 0.8470\n"
     ]
    }
   ],
   "source": [
    "''' Best logreg model for test set '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=3e-2,\n",
    "                        alpha=0.02, l1_ratio=0.12)\n",
    "\n",
    "pipe_log = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      ('SGDlog', sgd_log)])\n",
    "pipe_log.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(pipe_log, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train, pipe_log.predict_proba(X_train)[:,1]))\n",
    "print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, pipe_log.predict_proba(X_hold)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_log.fit(X, y)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = pipe_log.predict_proba(X_test)[:,1]\n",
    "submission.to_csv('submission_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Без выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.8834\n",
      "Cross-validation ROC AUC: mean 0.8264, std 0.0382\n",
      "Holdout ROC AUC: 0.8243\n"
     ]
    }
   ],
   "source": [
    "''' Logregr without outliers '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=1e-2,\n",
    "                        alpha=0.02, l1_ratio=0.12)\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD))])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_train_pre, y_train_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_train_pre,\n",
    "                                                                                 y_train)\n",
    "X_hold_pre = pipe_pre.transform(X_hold)\n",
    "\n",
    "sgd_log.fit(X_train_pre, y_train_pre)\n",
    "\n",
    "scores = cross_val_score(sgd_log, X_train_pre, y_train_pre, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train_pre, sgd_log.predict_proba(X_train_pre)[:,1]))\n",
    "print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, sgd_log.predict_proba(X_hold_pre)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_pre, y_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_pre, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "\n",
    "sgd_log.fit(X_pre, y_pre)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = sgd_log.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_log_no_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding вместо Target Mean Encoding - лучшая модель, не использующая ансамбли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.9153\n",
      "Cross-validation ROC AUC: mean 0.8391, std 0.0260\n",
      "Holdout ROC AUC: 0.8283\n"
     ]
    }
   ],
   "source": [
    "''' Logregr with one-hot '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=1e-2,\n",
    "                        alpha=0.02, l1_ratio=0.12)\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('le', LE_df(threshold=CORR_THRESHOLD)),\n",
    "                      ('ohe', OneHot_df(threshold=CAT_THRESHOLD, drop_collinear=True)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD))])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_train_pre, y_train_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_train_pre,\n",
    "                                                                                y_train)\n",
    "\n",
    "X_hold_pre = pipe_pre.transform(X_hold)\n",
    "\n",
    "sgd_log.fit(X_train_pre, y_train_pre)\n",
    "\n",
    "scores = cross_val_score(sgd_log, X_train_pre, y_train_pre, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train_pre, sgd_log.predict_proba(X_train_pre)[:,1]))\n",
    "print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, sgd_log.predict_proba(X_hold_pre)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param = ['all', 'no_sc', 'no_transform_no_sc', 'no_lootme_no_sc', 'le_insteadof_lootme_no_sc', 'tme -> ohe', 'tme -> ohe + dropout']\n",
    "\n",
    "CV_ROC_AUC = [0.7611, 0.8291, 0.6834, 0.8198, 0.8212, 0.8377, 0.8213]\n",
    "\n",
    "HOLD_ROC_AUC = [0.7891, 0.8363, 0.6123, 0.8494, 0.8442, 0.8459, 0.8322]\n",
    "\n",
    "Лучшая схема препроцессинга - c заменой tme на ohe. Выбрасывание выбросов - под вопросом, надо пробовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.88280 на тесте\n",
    "\n",
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_pre, y_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_pre, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "\n",
    "sgd_log.fit(X_pre, y_pre)\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = sgd_log.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_log_ohe_no_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection - SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [646 647 648 654 657 667 687 701 705 708 730 750 751 772 786 812 813 840\n",
      " 844 861 862 875 876 877 884 885 886 887 888 889] are constant.\n",
      "  UserWarning)\n",
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.8858\n",
      "Cross-validation ROC AUC: mean 0.8484, std 0.0265\n",
      "Holdout ROC AUC: 0.8270\n"
     ]
    }
   ],
   "source": [
    "''' SelectKBest for LogReg '''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "SELECT_COEF = 0.5\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=1e-2,\n",
    "                        alpha=0.02, l1_ratio=0.12)\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('le', LE_df(threshold=CORR_THRESHOLD)),\n",
    "                      ('ohe', OneHot_df(threshold=CAT_THRESHOLD, drop_collinear=True)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD))])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_train_pre, y_train_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_train_pre,\n",
    "                                                                                y_train)\n",
    "X_hold_pre = pipe_pre.transform(X_hold)\n",
    "\n",
    "K = int(np.floor(X_train_pre.shape[0]) * SELECT_COEF)\n",
    "\n",
    "feat_select = SelectKBest(f_classif, K)\n",
    "X_train_pre = feat_select.fit_transform(X_train_pre, y_train_pre)\n",
    "X_hold_pre = feat_select.transform(X_hold_pre)\n",
    "\n",
    "\n",
    "sgd_log.fit(X_train_pre, y_train_pre)\n",
    "\n",
    "scores = cross_val_score(sgd_log, X_train_pre, y_train_pre, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train_pre, sgd_log.predict_proba(X_train_pre)[:,1]))\n",
    "print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, sgd_log.predict_proba(X_hold_pre)[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попробовать использовать выбор первых 50% фич с текущей конфигурацией препроцессинга.\n",
    "\n",
    "* SELECT_COEF = [-, 1.0, 0.5, 0.25, 0.75, 0.6, 0.4, 0.55]\n",
    "* CV_ROC_AUC = [0.8391, 0.8425, 0.8477, 0.8432, 0.8461, 0.8479, 0.8434, 0.8468]\n",
    "* HOLD_ROC_AUC = [0.8263, 0.8232, 0.8271, 0.8078, 0.8233, 0.8260, 0.8229, 0.8236]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [646 647 648 657 705 708 750 751 812 813 876 877 884 885 886 887 888 889] are constant.\n",
      "  UserWarning)\n",
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# 0.87988 - база для бэггинговой модели\n",
    "\n",
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_pre, y_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_pre, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "\n",
    "K = int(np.floor(X_pre.shape[0]) * SELECT_COEF)\n",
    "\n",
    "feat_select = SelectKBest(f_classif, K)\n",
    "X_pre = feat_select.fit_transform(X_pre, y_pre)\n",
    "X_test_pre = feat_select.transform(X_test_pre)\n",
    "\n",
    "sgd_log.fit(X_pre, y_pre)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = sgd_log.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_log_ohe_no_out_kbest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бэггинг лучших моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.8926\n",
      "Holdout ROC AUC: 0.8341\n",
      "CPU times: user 15.2 s, sys: 612 ms, total: 15.8 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' Bagging of the Log Reg - wihtout feature selector '''\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=1e-2,\n",
    "                        alpha=0.005, l1_ratio=0.12)\n",
    "bag_sgd_log = BaggingClassifier(sgd_log, n_estimators=50, random_state=17, max_features=0.2)\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('le', LE_df(threshold=CORR_THRESHOLD)),\n",
    "                      ('ohe', OneHot_df(threshold=CAT_THRESHOLD, drop_collinear=True)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD))])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_train_pre, y_train_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_train_pre,\n",
    "                                                                                y_train)\n",
    "\n",
    "X_hold_pre = pipe_pre.transform(X_hold)\n",
    "\n",
    "bag_sgd_log.fit(X_train_pre, y_train_pre)\n",
    "\n",
    "#scores = cross_val_score(sgd_log, X_train_pre, y_train_pre, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train_pre, bag_sgd_log.predict_proba(X_train_pre)[:,1]))\n",
    "#print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, bag_sgd_log.predict_proba(X_hold_pre)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.87742\n",
    "\n",
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_pre, y_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_pre, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "\n",
    "sgd_log.fit(X_pre, y_pre)\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = sgd_log.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_bag_log_ohe_no_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Лучшая модель с логистической регрессией "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [646 647 648 654 657 667 687 701 705 708 730 750 751 772 786 812 813 840\n",
      " 844 861 862 875 876 877 884 885 886 887 888 889] are constant.\n",
      "  UserWarning)\n",
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.8938\n",
      "Holdout ROC AUC: 0.8295\n",
      "CPU times: user 15.5 s, sys: 652 ms, total: 16.1 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' Bagging of the LogReg - with feature selector '''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "SELECT_COEF = 0.5\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log', n_jobs=-1, learning_rate='optimal', tol=1e-2,\n",
    "                        alpha=0.005, l1_ratio=0.12)\n",
    "bag_sgd_log = BaggingClassifier(sgd_log, n_estimators=50, random_state=17, max_features=0.5)\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('le', LE_df(threshold=CORR_THRESHOLD)),\n",
    "                      ('ohe', OneHot_df(threshold=CAT_THRESHOLD, drop_collinear=True)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD))])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_train_pre, y_train_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_train_pre,\n",
    "                                                                                y_train)\n",
    "X_hold_pre = pipe_pre.transform(X_hold)\n",
    "\n",
    "K = int(np.floor(X_train_pre.shape[0]) * SELECT_COEF)\n",
    "\n",
    "feat_select = SelectKBest(f_classif, K)\n",
    "X_train_pre = feat_select.fit_transform(X_train_pre, y_train_pre)\n",
    "X_hold_pre = feat_select.transform(X_hold_pre)\n",
    "\n",
    "\n",
    "bag_sgd_log.fit(X_train_pre, y_train_pre)\n",
    "\n",
    "#scores = cross_val_score(bag_sgd_log, X_train_pre, y_train_pre, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train_pre, bag_sgd_log.predict_proba(X_train_pre)[:,1]))\n",
    "#print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, bag_sgd_log.predict_proba(X_hold_pre)[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [646 647 648 657 705 708 750 751 812 813 876 877 884 885 886 887 888 889] are constant.\n",
      "  UserWarning)\n",
      "/Users/A.Miroshnikova/Public/Conda/anaconda3/envs/opencv/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# 0.88549\n",
    "\n",
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_pre, y_pre = DetectOut_df(out_fraction=OUT_FRACTION).fit_transform(X_pre, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "\n",
    "K = int(np.floor(X_pre.shape[0]) * SELECT_COEF)\n",
    "\n",
    "feat_select = SelectKBest(f_classif, K)\n",
    "X_pre = feat_select.fit_transform(X_pre, y_pre)\n",
    "X_test_pre = feat_select.transform(X_test_pre)\n",
    "\n",
    "bag_sgd_log.fit(X_pre, y_pre)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = bag_sgd_log.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_bag_log_ohe_no_out_kbest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
