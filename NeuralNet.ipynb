{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 2000) # since we have a lot of features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import data '''\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "train_file = os.path.join(DATA_DIR, 'train.csv')\n",
    "test_file = os.path.join(DATA_DIR, 'test.csv')\n",
    "submission_file = os.path.join(DATA_DIR, 'sample_submission.csv')\n",
    "\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "X = train.drop(columns=['y', 'sample_id'])\n",
    "y = train.y\n",
    "X_test = test.drop(columns=['sample_id'])\n",
    "\n",
    "X_all = X.append(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(766, 1777)\n",
      "(766, 932)\n",
      "(766, 1908)\n",
      "(766, 1908)\n",
      "(329, 2518)\n",
      "(329, 1908)\n",
      "(727, 1908)\n"
     ]
    }
   ],
   "source": [
    "''' Gather all preprocessing functions '''\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import bisect\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "''' 1 - divide to training and holdout datasets '''\n",
    "\n",
    "X_train, X_hold, y_train, y_hold = train_test_split(X, y, random_state=3, test_size=0.3)\n",
    "\n",
    "''' 2 - define the same validation scheme for all models '''\n",
    "\n",
    "skf = RepeatedStratifiedKFold(n_repeats=5, random_state=17)\n",
    "\n",
    "\n",
    "''' Function 1 - delete all columns with NaN or Inf '''\n",
    "columns_to_keep = X_all.replace([np.inf, -np.inf], np.nan).dropna(axis=1).columns\n",
    "\n",
    "def del_nan(x):\n",
    "    return x[columns_to_keep]\n",
    "delete_nan = FunctionTransformer(del_nan, validate=False)\n",
    "\n",
    "\n",
    "''' Function 2 - delete NaN-only columns '''\n",
    "\n",
    "na_cols = X_all[X_all.isna().any()[lambda x: x].index].isna().agg(['sum', 'count']).T\n",
    "\n",
    "all_na_cols = na_cols[na_cols['sum'] == na_cols['count']]\n",
    "\n",
    "def drop_all_na(X):\n",
    "    ''' Here we drop all columns which consist of NaN only '''\n",
    "    idx = all_na_cols.index.values\n",
    "    return X.drop(columns=idx)\n",
    "\n",
    "drop_all_na_f = FunctionTransformer(drop_all_na, validate=False)\n",
    "\n",
    "\n",
    "''' Function 3 - replace NaN '''\n",
    "\n",
    "has_na_cols = na_cols.T.drop(columns=all_na_cols.index.values).T\n",
    "has_na_cols['perc'] = has_na_cols['sum']/has_na_cols['count']\n",
    "\n",
    "'''def replace_na(X, add_boolean=True, threshold=0.3):\n",
    "    Here we apply two filling strategies\n",
    "            for two types of columns with na:\n",
    "            % na > threshold - fill with mean\n",
    "            % na < threshold - fill with 0.\n",
    "        Plus we add additional columns showing\n",
    "            if the value was na before transformation.\n",
    "    X_copy = X.copy()\n",
    "    cols_less = has_na_cols[has_na_cols['perc'] < threshold].index.values\n",
    "    cols_more = has_na_cols[has_na_cols['perc'] >= threshold].index.values\n",
    "    X_less = X_copy[cols_less]\n",
    "    X_less = X_less.fillna(X_less.mean())\n",
    "    X_copy[cols_less] = X_less\n",
    "    X_copy[cols_more] = X_more\n",
    "    if add_boolean:\n",
    "        was_na = X[has_na_cols.index.values].isna().astype('int64')\n",
    "        was_na.columns = was_na.columns.map(lambda x: x + '_na')\n",
    "        X_copy = pd.concat([X_copy, was_na], axis=1)\n",
    "    return X_copy'''\n",
    "    \n",
    "# I should have checked columns for categorical features before\n",
    "\n",
    "def replace_na(X, add_boolean=True):\n",
    "    ''' Here we apply mean filling strategy.\n",
    "        And add additional columns showing if the values\n",
    "        was na before transformation.\n",
    "    '''\n",
    "    X_copy = X.copy()\n",
    "    cols = has_na_cols.index.values\n",
    "    X_c = X_copy[cols]\n",
    "    X_c = X_c.fillna(X_all.replace([np.inf, -np.inf], np.nan).mean())\n",
    "    X_copy[cols] = X_c\n",
    "    if add_boolean:\n",
    "        was_na = X[has_na_cols.index.values].isna().astype('int64')\n",
    "        was_na.columns = was_na.columns.map(lambda x: x + '_na')\n",
    "        X_copy = pd.concat([X_copy, was_na], axis=1)\n",
    "    return X_copy\n",
    "\n",
    "replace_na_f = FunctionTransformer(replace_na, validate=False)\n",
    "\n",
    "\n",
    "''' Function 4 - replace inf '''\n",
    "\n",
    "pos_inf_cols = X_all.fillna(0).replace([np.inf], \n",
    "                                       np.nan).isna().any()[lambda x: x].index\n",
    "pos_inf_cols = X_all[pos_inf_cols].replace([np.inf], \n",
    "                                           np.nan).isna().agg(['sum', 'count']).T\n",
    "neg_inf_cols = X_all.fillna(0).replace([-np.inf],\n",
    "                                       np.nan).isna().any()[lambda x: x].index\n",
    "\n",
    "def fill_inf(X, add_boolean=True, multiplier=2):\n",
    "    '''\n",
    "    Here we apply filling strategy for infinite values:\n",
    "        we replace it with absolute maximum value * multiplier.\n",
    "    Additionally, we new columns showing if the value\n",
    "        was infinite before filling.\n",
    "    '''\n",
    "    X_copy = X.copy()\n",
    "    X_pos = X_copy[pos_inf_cols.index.values]\n",
    "    to_fill = X_pos.replace(np.inf, np.nan).abs().max() * multiplier + 1.0\n",
    "    X_pos = X_pos.replace(np.inf, np.nan).fillna(to_fill)\n",
    "    X_copy[pos_inf_cols.index.values] = X_pos\n",
    "    if add_boolean:\n",
    "        was_inf = X[pos_inf_cols.index.values].replace(np.inf, np.nan).isna().astype('int64')\n",
    "        was_inf.columns = was_inf.columns.map(lambda x: x + '_inf')\n",
    "        X_copy = pd.concat([X_copy, was_inf], axis=1)\n",
    "    return X_copy\n",
    "\n",
    "fill_inf_f = FunctionTransformer(fill_inf, validate=False)\n",
    "\n",
    "\n",
    "''' First block of data cleaning is gathered in one function '''\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "\n",
    "''' Function and class 5 - delete zero-variance features '''\n",
    "\n",
    "w = fill_inf(replace_na(drop_all_na(X_all))).var()\n",
    "zero_var_cols = w[np.isclose(w, 0)]\n",
    "\n",
    "def drop_zero_var(X):\n",
    "    idx = zero_var_cols.index.values\n",
    "    return X.drop(columns=idx)\n",
    "\n",
    "drop_zero_var_f = FunctionTransformer(drop_zero_var)\n",
    "\n",
    "class Drop_zero_var:\n",
    "    def __init__(self):\n",
    "        self.zero_var_cols = []\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        w = X.var()\n",
    "        self.zero_var_cols = w[np.isclose(w, 0.0)].index.values\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        return X_copy.drop(columns=self.zero_var_cols)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(Drop_zero_var().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "\n",
    "''' Function and class 6 - drop highly correlated features '''\n",
    "\n",
    "'''\n",
    "# Here is the function\n",
    "print(drop_zero_var(pre_proc(X_all)).shape)\n",
    "corr_matrix = pre_proc(X_all).corr().abs()\n",
    "print(corr_matrix.shape)\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(len(to_drop))\n",
    "\n",
    "'''\n",
    "\n",
    "class Drop_corr:\n",
    "    def __init__(self, threshold=0.95):\n",
    "        self.to_drop = []\n",
    "        self.is_fitted = False\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        corr_matrix = X.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        self.to_drop = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        return X_copy.drop(columns=self.to_drop)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(Drop_corr().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "    \n",
    "''' Class 7 - encode categorical labels '''\n",
    "\n",
    "class LE_df:\n",
    "    def __init__(self, threshold=5):\n",
    "        self.le_dict = {}\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        categorical = X_copy[counts[counts <= self.threshold].index]\n",
    "        for col in categorical.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(categorical[col])\n",
    "            self.le_dict[col] = le\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.le_dict.keys():\n",
    "            X_copy[col] = X_copy[col].map(lambda s: -100500 if s not in \\\n",
    "                                          self.le_dict[col].classes_ else s)\n",
    "            if np.any(X_copy[col]==-100500):\n",
    "                le_classes = self.le_dict[col].classes_.tolist()\n",
    "                le_classes.append(-100500)\n",
    "                self.le_dict[col].classes_ = np.asarray(le_classes)\n",
    "            X_copy[col] = self.le_dict[col].transform(X_copy[col])\n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "print(LE_df().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "    \n",
    "''' Class 8 - target mean encode categorical labels '''\n",
    "\n",
    "class LOOTME_df:\n",
    "    ''' Class performs leave-one-out target mean encoding '''\n",
    "    def __init__(self, threshold=5):\n",
    "        self.threshold = threshold\n",
    "        self.tme_dict = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_copy = X.copy()\n",
    "        y_copy = y.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        categorical = X_copy[counts[counts <= self.threshold].index]\n",
    "        for col in categorical.columns:\n",
    "            df = pd.concat((X_copy[col], y_copy), axis=1)\n",
    "            df.columns = ['col', 'y']\n",
    "            tme = {}\n",
    "            for value in df['col'].unique():\n",
    "                y_val = df[df['col'] == value]['y'].values\n",
    "                col_val = df[df['col'] == value]['col'].values\n",
    "                loo_mean = np.mean((np.full_like(col_val, y_val.sum()) - y_val) / y_val.shape[0])\n",
    "                tme[value] = loo_mean\n",
    "            self.tme_dict[col] = tme\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.tme_dict.keys():\n",
    "            X_copy[col] = X_copy[col].map(lambda s: -100500 if s not in self.tme_dict[col].keys() else s)\n",
    "            self.tme_dict[col][-100500] = 0.0\n",
    "            X_copy[col] = X_copy[col].map(self.tme_dict[col])\n",
    "        return X_copy\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "    \n",
    "print(LOOTME_df().fit_transform(pre_proc(X_train), y_train).shape)\n",
    "\n",
    "\n",
    "''' Class 9 - One Hot Encoding of categorical features '''\n",
    "\n",
    "class OneHot_df:\n",
    "    def __init__(self, threshold=5, drop_collinear=False):\n",
    "        self.threshold = threshold\n",
    "        self.drop_collinear = drop_collinear\n",
    "        self.categorical = []\n",
    "        self.ohe = None\n",
    "        self.column_names = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        X_cat = X_copy[counts[(counts <= self.threshold) & (counts > 1)].index]\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        ohe.fit(X_cat)\n",
    "        self.categorical = X_cat.columns\n",
    "        self.ohe = ohe\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_cat = X_copy.copy()[self.categorical]\n",
    "        X_cat_tr = self.ohe.transform(X_cat).toarray()\n",
    "        if not self.column_names:\n",
    "            column_names = []\n",
    "            for i, col in enumerate(self.categorical):\n",
    "                column_names.extend([col + '_' + str(s) for s in self.ohe.categories_[i]])\n",
    "            self.column_names = column_names\n",
    "        X_cat_tr = pd.DataFrame(X_cat_tr, columns=self.column_names, index=X_copy.index)\n",
    "        X_other = X_copy.drop(columns=self.categorical)\n",
    "        X_copy = pd.concat([X_other, X_cat_tr], axis=1)\n",
    "        if self.drop_collinear:\n",
    "            to_drop = pd.Series(self.column_names, \n",
    "        index=self.column_names).apply(lambda x: x.endswith('_0'))[lambda x: x].index\n",
    "            X_copy.drop(columns=to_drop)\n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(OneHot_df().fit(pre_proc(X_train)).transform(pre_proc(X_hold)).shape)\n",
    "\n",
    "''' Class 10 - find best normalizing transformation '''\n",
    "\n",
    "def get_shapiro_p(X):\n",
    "    _, p = shapiro(X)\n",
    "    return p\n",
    "\n",
    "class Find_Trans:\n",
    "    def __init__(self, threshold=10):\n",
    "        self.threshold = threshold\n",
    "        self.pvals = []\n",
    "        self.numerical = []\n",
    "        self.is_fitted = False\n",
    "        self.mms = MinMaxScaler()\n",
    "        self.trans_dict = {'no': lambda x: x,\n",
    "                           'x^2': lambda x: np.power(x, 2), \n",
    "                           'x^3': lambda x: np.power(x, 3), \n",
    "                           'log(x)': lambda x: np.log(x), \n",
    "                           #'exp(x)': lambda x: np.exp(x/10), \n",
    "                           'sqrt(x)': lambda x: np.sqrt(x)}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        counts = X_copy.apply(lambda x: x.value_counts().shape[0])\n",
    "        self.numerical = counts[counts > self.threshold].index.values\n",
    "        X_numer = X_copy[self.numerical]\n",
    "        idx = X_numer.index\n",
    "        X_numer = pd.DataFrame(self.mms.fit_transform(X_numer) + 0.01,\n",
    "                               columns=self.numerical,\n",
    "                               index=idx)\n",
    "        pvals = pd.DataFrame(X_numer.apply(get_shapiro_p), columns=['no'])\n",
    "        for name in self.trans_dict.keys():\n",
    "            if name not in pvals.columns:\n",
    "                pvals[name] = X_numer.apply(self.trans_dict[name]).apply(get_shapiro_p)\n",
    "        self.pvals = pvals\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_numer = X_copy[self.numerical]\n",
    "        idx = X_numer.index\n",
    "        X_numer = pd.DataFrame(self.mms.transform(X_numer) + 0.01,\n",
    "                               columns=self.numerical,\n",
    "                               index=idx)\n",
    "        best_transform = self.pvals.idxmax(axis=1)\n",
    "        best_transform = best_transform.map(self.trans_dict).to_dict()\n",
    "        X_numer = X_numer.apply(best_transform)\n",
    "        X_numer = X_numer.fillna(X_numer.min()-1)\n",
    "        for col in self.numerical:\n",
    "            X_copy[col] = X_numer[col]\n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "print(Find_Trans().fit(pre_proc(X_train), y_train).transform(pre_proc(X_hold)).shape)\n",
    "\n",
    "\n",
    "''' Class 11 - find and drop outliers '''\n",
    "\n",
    "class DetectOut_df:\n",
    "    def __init__(self, method='IsolationForest', out_fraction=0.05, \n",
    "                 drop=True, random_state=17,\n",
    "                 kernel='rbf', shrink=True):\n",
    "        self.method = method\n",
    "        self.out_fraction = out_fraction\n",
    "        self.drop = drop\n",
    "        self.is_fitted = False\n",
    "        self.random_state = random_state\n",
    "        self.kernel = kernel\n",
    "        self.shrink = shrink\n",
    "        self.model = None\n",
    "        self.train_set = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'IsolationForest':\n",
    "            model = IsolationForest(random_state=self.random_state,\n",
    "                        n_estimators=100,\n",
    "                        max_features=0.9,\n",
    "                        contamination=self.out_fraction, #\n",
    "                        n_jobs=-1,\n",
    "                        behaviour='new')\n",
    "        if self.method == 'OneClassSVM':\n",
    "            model = OneClassSVM(kernel=self.kernel,\n",
    "                     nu=self.out_fraction,\n",
    "                     shrinking=self.shrink,\n",
    "                     gamma='scale'\n",
    "                    )\n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        self.is_fitted = True\n",
    "        self.train_set = X.copy()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        ''' We shouldn't predict any outliers for test set '''\n",
    "        X_copy = X.copy()\n",
    "        X_copy['outlier'] = 1\n",
    "        X_copy['y'] = y.copy()\n",
    "        if self.train_set.equals(X):\n",
    "            X_copy['outlier'] = self.model.predict(X)\n",
    "        if self.drop:\n",
    "            X_copy = X_copy[X_copy['outlier'] == 1]\n",
    "            X_copy = X_copy.drop(columns=['outlier'])\n",
    "        y_copy = X_copy['y']\n",
    "        X_copy = X_copy.drop(columns=['y'])\n",
    "        return X_copy, y_copy\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        if not self.is_fitted:\n",
    "            self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "    \n",
    "print(DetectOut_df().fit_transform(pre_proc(X_train), y_train)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель 1 - после трансформации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 486 ms, total: 23.9 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' Prepare the data '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      ('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      ('sc', StandardScaler())\n",
    "                    ])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_hold_pre = pipe_pre.transform(X_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.9576\n",
      "Holdout ROC AUC: 0.8560\n",
      "CPU times: user 15.4 s, sys: 2.16 s, total: 17.6 s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow import ConfigProto, Session\n",
    "from tensorflow.keras.backend import set_session\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "'''config = ConfigProto(device_count={'GPU': 1, 'CPU': 2}) \n",
    "sess = Session(config=config) \n",
    "set_session(sess)'''\n",
    "\n",
    "set_random_seed(17)\n",
    "INPUT_DIM = X_train_pre.shape[1]\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "VAL_SPLIT = 0.0\n",
    "L2_REG = 4e-4\n",
    "DROPOUT_RATE = 0.15\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "def make_dense(layers=3, shapes=(32, 32, 32), activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=shapes[0], input_dim=INPUT_DIM))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation=activation))\n",
    "    model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    for i in range(1, layers):\n",
    "        model.add(Dense(units=shapes[i]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation))\n",
    "        model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(L2_REG)))\n",
    "    model.compile(optimizer=Adam(lr=LEARNING_RATE, decay=1e-3),\n",
    "                  loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(make_dense, validation_split=VAL_SPLIT, shuffle=True,\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n",
    "\n",
    "model.fit(X_train_pre, y_train)\n",
    "\n",
    "#scores = cross_val_score(model, X_train_pre, y_train, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train, model.predict_proba(X_train_pre)[:,1]))\n",
    "#print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, model.predict_proba(X_hold_pre)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "model.fit(X_pre, y)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = model.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_nn_3x32_all_sc.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Без трансформации фич - лучшая модель, 0.87148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 437 ms, total: 17.4 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' Prepare the data '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('lootme', LOOTME_df(threshold=CAT_THRESHOLD)),\n",
    "                      #('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      ('sc', StandardScaler())\n",
    "                    ])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_hold_pre = pipe_pre.transform(X_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.9456\n",
      "Holdout ROC AUC: 0.8245\n",
      "CPU times: user 14.1 s, sys: 1.04 s, total: 15.1 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow import ConfigProto, Session\n",
    "from tensorflow.keras.backend import set_session\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "config = ConfigProto(device_count={'GPU': 1, 'CPU': 2}) \n",
    "sess = Session(config=config) \n",
    "set_session(sess)\n",
    "\n",
    "set_random_seed(17)\n",
    "INPUT_DIM = X_train_pre.shape[1]\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "VAL_SPLIT = 0.0\n",
    "L2_REG = 4e-4\n",
    "DROPOUT_RATE = 0.15\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "def make_dense(layers=3, shapes=(32, 32, 32), activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=shapes[0], input_dim=INPUT_DIM))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation=activation))\n",
    "    model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    for i in range(1, layers):\n",
    "        model.add(Dense(units=shapes[i]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation))\n",
    "        model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(L2_REG)))\n",
    "    model.compile(optimizer=Adam(lr=LEARNING_RATE, decay=1e-3),\n",
    "                  loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(make_dense, validation_split=VAL_SPLIT, shuffle=True,\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n",
    "\n",
    "model.fit(X_train_pre, y_train)\n",
    "\n",
    "#scores = cross_val_score(model, X_train_pre, y_train, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train, model.predict_proba(X_train_pre)[:,1]))\n",
    "#print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, model.predict_proba(X_hold_pre)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "model.fit(X_pre, y)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = model.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_nn_3x32_all_sc_no_trans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 455 ms, total: 15.7 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' Neural Net with One Hot encoded categorical features '''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CORR_THRESHOLD = 0.94\n",
    "CAT_THRESHOLD = 5 # <- and less unique values make feature categorical\n",
    "OUT_FRACTION = 0.05\n",
    "\n",
    "def pre_proc(X):\n",
    "    return fill_inf(replace_na(drop_all_na(X)))\n",
    "\n",
    "pipe_pre = Pipeline([('pre_proc1', FunctionTransformer(pre_proc, validate=False)),\n",
    "                      ('drop_zero_var', Drop_zero_var()),\n",
    "                      ('drop_corr', Drop_corr(threshold=CORR_THRESHOLD)),\n",
    "                      ('le', LE_df(threshold=CORR_THRESHOLD)),\n",
    "                      ('ohe', OneHot_df(threshold=CAT_THRESHOLD)),\n",
    "                      #('transform', Find_Trans(threshold=CAT_THRESHOLD)),\n",
    "                      ('sc', StandardScaler())\n",
    "                    ])\n",
    "\n",
    "X_train_pre = pipe_pre.fit_transform(X_train, y_train)\n",
    "X_hold_pre = pipe_pre.transform(X_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.9697\n",
      "Holdout ROC AUC: 0.8340\n",
      "CPU times: user 19.2 s, sys: 1.63 s, total: 20.8 s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow import ConfigProto, Session\n",
    "from tensorflow.keras.backend import set_session, clear_session\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from numba import cuda\n",
    "\n",
    "config = ConfigProto(device_count={'GPU': 1, 'CPU': 2}) \n",
    "sess = Session(config=config) \n",
    "set_session(sess)\n",
    "\n",
    "set_random_seed(17)\n",
    "INPUT_DIM = X_train_pre.shape[1]\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "VAL_SPLIT = 0.0\n",
    "L2_REG = 4e-4\n",
    "DROPOUT_RATE = 0.15\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "def make_dense(layers=3, shapes=(32, 32, 32), activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=shapes[0], input_dim=INPUT_DIM))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation=activation))\n",
    "    model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    for i in range(1, layers):\n",
    "        model.add(Dense(units=shapes[i]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation=activation))\n",
    "        model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(L2_REG)))\n",
    "    model.compile(optimizer=Adam(lr=LEARNING_RATE, decay=2e-3),\n",
    "                  loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(make_dense, validation_split=VAL_SPLIT, shuffle=True,\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n",
    "\n",
    "model.fit(X_train_pre, y_train)\n",
    "\n",
    "#scores = cross_val_score(model, X_train_pre, y_train, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print('Train ROC AUC: %.4f' % roc_auc_score(y_train, model.predict_proba(X_train_pre)[:,1]))\n",
    "#print('Cross-validation ROC AUC: mean %.4f, std %.4f' % (scores.mean(), scores.std()))\n",
    "print('Holdout ROC AUC: %.4f' % roc_auc_score(y_hold, model.predict_proba(X_hold_pre)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre = pipe_pre.fit_transform(X, y)\n",
    "X_test_pre = pipe_pre.transform(X_test)\n",
    "model.fit(X_pre, y)\n",
    "\n",
    "submission = pd.read_csv(submission_file)\n",
    "submission['y'] = model.predict_proba(X_test_pre)[:,1]\n",
    "submission.to_csv('submission_nn_3x32_all_sc_no_trans_no_tme_ohe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
